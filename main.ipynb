{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "\n",
            "def set_parser(data):\n",
            "  try:\n",
            "    j1 = list(data.strip('*}*{').split(','))\n",
            "    return j1\n",
            "  except:\n",
            "    return 'problem'\n",
            "\n",
            "df = pd.read_csv('train.csv', converters={'amenities':set_parser}, header=0)\n",
            "# df = pd.read_csv('train.csv')\n",
            "\n",
            "y = df[['price']]\n",
            "\n",
            "\n",
            "# Can make additional columns attributes for amenities\n",
            "#df[['amenities']]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "setA = []\n",
            "count = 0\n",
            "for B in df[['amenities']].values:\n",
            "  setA.append(B)\n",
            "\n",
            "print(np.unique(setA))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df.columns"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 87,
         "metadata": {},
         "outputs": [],
         "source": [
            "df.to_csv('train_w_amenities.csv')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import json\n",
            "from math import isnan\n",
            "\n",
            "def to_float(s):\n",
            "    try:\n",
            "        ret = float(s)\n",
            "    except:\n",
            "        ret = -1\n",
            "    if isnan(ret):\n",
            "        ret = -1\n",
            "    return ret\n",
            "\n",
            "def to_float_avg(s, name_of_col, data_frame):\n",
            "    avg = data_frame[name_of_col].mean()\n",
            "    try:\n",
            "        ret = float(s)\n",
            "    except:\n",
            "        ret = avg\n",
            "    if isnan(ret):\n",
            "        ret = avg\n",
            "    return ret\n",
            "\n",
            "def to_int(s):\n",
            "    try:\n",
            "        ret = int(s)\n",
            "    except:\n",
            "        ret = -1\n",
            "    return ret\n",
            "\n",
            "def to_int_avg(s, name_of_col, data_frame):\n",
            "    avg = data_frame[name_of_col].mean()\n",
            "    try:\n",
            "        ret = int(s)\n",
            "    except:\n",
            "        ret = avg\n",
            "    return ret\n",
            "\n",
            "def json_parser(data):\n",
            "    j1 = json.loads(data)\n",
            "    return j1\n",
            "\n",
            "categories = [x for x in list(set(df['neighbourhood_group_cleansed'])) if type(x) is str]\n",
            "state_categories = [x for x in list(set(df['state'])) if type(x) is str]\n",
            "state_categories = [x for x in list(set(df['state'])) if type(x) is str]\n",
            "room_categories = [x for x in list(set(df['room_type'])) if type(x) is str]\n",
            "super_host_categories = [x for x in list(set(df['host_is_superhost'])) if type(x) is str]\n",
            "bed_type_categories = [x for x in list(set(df['bed_type'])) if type(x) is str]\n",
            "cancellation_policy_categories = [x for x in list(set(df['cancellation_policy'])) if type(x) is str]\n",
            "property_type_categories = [x for x in list(set(df['property_type'])) if type(x) is str]\n",
            "print(len(categories))\n",
            "\n",
            "def create_feature(row, data_frame):\n",
            "    guest_num = to_int(row.guests_included)\n",
            "    guest_cap = to_float(row.extra_people)\n",
            "    bedrooms = to_int(row.bedrooms)\n",
            "    bathrooms = to_int(row.bathrooms)\n",
            "    beds = to_int(row.beds)\n",
            "    review = to_float(row.review_scores_rating)\n",
            "    review_scores_location = to_float(row.review_scores_location)\n",
            "    cleanliness_review = to_int(row.review_scores_cleanliness)\n",
            "    number_of_reviews = to_int(row.number_of_reviews)\n",
            "    host_listings = to_int(row.calculated_host_listings_count)\n",
            "    accommodates = to_int(row.accommodates)\n",
            "    amenities = to_int(row.amenities)\n",
            "    reviews_per_month = to_float(row.reviews_per_month)\n",
            "    one_hot = [int(row.neighbourhood_group_cleansed == category) for category in categories]\n",
            "    two_hot = [int(row.state == state_category) for state_category in state_categories]\n",
            "    three_hot = [int(row.room_type == room_category) for room_category in room_categories]\n",
            "    four_hot = [int(row.host_is_superhost == super_host_category) for super_host_category in super_host_categories]\n",
            "    five_hot = [int(row.bed_type == bed_type_category) for bed_type_category in bed_type_categories]\n",
            "    six_hot = [int(row.cancellation_policy == cancelation_policy_category) for cancelation_policy_category in cancellation_policy_categories]\n",
            "    # seven_hot = [int(row.property_type == property_type_category) for property_type_category in property_type_categories]\n",
            "    \n",
            "    return [guest_num, guest_cap, accommodates, bedrooms, bathrooms, beds, number_of_reviews, reviews_per_month] + one_hot + three_hot + four_hot \n",
            "\n",
            "\n",
            "train_X, train_y = [], []\n",
            "\n",
            "for (idx, row) in df.iterrows():\n",
            "    price = to_float(row.price)\n",
            "    feature = create_feature(row, df)\n",
            "    #if row.accommodates == -1:\n",
            "    #    continue\n",
            "    #elif row.room_type == -1:\n",
            "    #    continue\n",
            "\n",
            "    train_X.append(feature)\n",
            "    train_y.append(price)\n",
            "\n",
            "print(len(train_X), len(train_y))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "5\n",
                  "23477 23477\n"
               ]
            }
         ],
         "source": [
            "import json\n",
            "from math import isnan\n",
            "\n",
            "def to_float(s):\n",
            "    try:\n",
            "        ret = float(s)\n",
            "    except:\n",
            "        ret = -1\n",
            "    if isnan(ret):\n",
            "        ret = -1\n",
            "    return ret\n",
            "\n",
            "def to_float_avg(s, name_of_col, data_frame):\n",
            "    avg = data_frame[name_of_col].mean()\n",
            "    try:\n",
            "        ret = float(s)\n",
            "    except:\n",
            "        ret = avg\n",
            "    if isnan(ret):\n",
            "        ret = avg\n",
            "    return ret\n",
            "\n",
            "def to_int(s):\n",
            "    try:\n",
            "        ret = int(s)\n",
            "    except:\n",
            "        ret = -1\n",
            "    return ret\n",
            "\n",
            "def to_int_avg(s, name_of_col, data_frame):\n",
            "    avg = data_frame[name_of_col].mean()\n",
            "    try:\n",
            "        ret = int(s)\n",
            "    except:\n",
            "        ret = avg\n",
            "    return ret\n",
            "\n",
            "def json_parser(data):\n",
            "    j1 = json.loads(data)\n",
            "    return j1\n",
            "\n",
            "categories = [x for x in list(set(df['neighbourhood_group_cleansed'])) if type(x) is str]\n",
            "state_categories = [x for x in list(set(df['state'])) if type(x) is str]\n",
            "state_categories = [x for x in list(set(df['state'])) if type(x) is str]\n",
            "room_categories = [x for x in list(set(df['room_type'])) if type(x) is str]\n",
            "super_host_categories = [x for x in list(set(df['host_is_superhost'])) if type(x) is str]\n",
            "bed_type_categories = [x for x in list(set(df['bed_type'])) if type(x) is str]\n",
            "cancellation_policy_categories = [x for x in list(set(df['cancellation_policy'])) if type(x) is str]\n",
            "property_type_categories = [x for x in list(set(df['property_type'])) if type(x) is str]\n",
            "print(len(categories))\n",
            "\n",
            "def create_feature(row, data_frame):\n",
            "    guest_num = to_int(row.guests_included)\n",
            "    guest_cap = to_float(row.extra_people)\n",
            "    bedrooms = to_int(row.bedrooms)\n",
            "    bathrooms = to_float(row.bathrooms)\n",
            "    beds = to_int(row.beds)\n",
            "    review = to_float(row.review_scores_rating)\n",
            "    review_scores_location = to_float(row.review_scores_location)\n",
            "    cleanliness_review = to_int(row.review_scores_cleanliness)\n",
            "    number_of_reviews = to_int(row.number_of_reviews)\n",
            "    host_listings = to_int(row.calculated_host_listings_count)\n",
            "    accommodates = to_int(row.accommodates)\n",
            "    amenities = to_int(row.amenities)\n",
            "    reviews_per_month = to_float(row.reviews_per_month)\n",
            "    reviews_value = to_float(row.review_scores_value)\n",
            "    review_accuracy = to_float(row.review_scores_accuracy)\n",
            "    one_hot = [int(row.neighbourhood_group_cleansed == category) for category in categories]\n",
            "    two_hot = [int(row.state == state_category) for state_category in state_categories]\n",
            "    three_hot = [int(row.room_type == room_category) for room_category in room_categories]\n",
            "    four_hot = [int(row.host_is_superhost == super_host_category) for super_host_category in super_host_categories]\n",
            "    five_hot = [int(row.bed_type == bed_type_category) for bed_type_category in bed_type_categories]\n",
            "    six_hot = [int(row.cancellation_policy == cancelation_policy_category) for cancelation_policy_category in cancellation_policy_categories]\n",
            "    seven_hot = [int(row.property_type == property_type_category) for property_type_category in property_type_categories]\n",
            "    \n",
            "    return [guest_num, guest_cap, accommodates, bathrooms, beds, bedrooms, review_scores_location, host_listings, number_of_reviews, reviews_per_month, review_accuracy, reviews_value] + one_hot + three_hot + four_hot + seven_hot\n",
            "\n",
            "\n",
            "train_X, train_y = [], []\n",
            "\n",
            "for (idx, row) in df.iterrows():\n",
            "    price = to_float(row.price)\n",
            "    feature = create_feature(row, df)\n",
            "    #if row.accommodates == -1:\n",
            "    #    continue\n",
            "    #elif row.room_type == -1:\n",
            "    #    continue\n",
            "\n",
            "    train_X.append(feature)\n",
            "    train_y.append(price)\n",
            "\n",
            "print(len(train_X), len(train_y))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.neural_network import MLPRegressor\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.model_selection import cross_val_score\n",
            "import numpy as np\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "# This is for finding best alpha value\n",
            "# 1.5 seems to a be a good value\n",
            "import matplotlib.pyplot as plt\n",
            "scaler_X = StandardScaler().fit(train_X)\n",
            "scaler_X.transform(train_X)\n",
            "\n",
            "scores = []\n",
            "alphas = []\n",
            "for i in np.linspace(1.5, 4.0, 20):\n",
            "  print(i)\n",
            "  score = cross_val_score(MLPRegressor(activation='logistic', alpha=i, hidden_layer_sizes=(8,), random_state=1, max_iter=1600), train_X, train_y, cv=3, n_jobs=8)\n",
            "  scores.append(np.mean(score))\n",
            "  alphas.append(i) \n",
            "plt.plot(alphas, scores)\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "0.4941976417175944\n"
               ]
            }
         ],
         "source": [
            "\n",
            "\n",
            "# This cell is used for cross validation of the model\n",
            "\n",
            "# scaler_X = StandardScaler().fit(train_X)\n",
            "# scaler_X.transform(train_X)\n",
            "\n",
            "# With standardized X\n",
            "# Setting up the neural net classifier\n",
            "regr = MLPRegressor(activation='logistic', alpha=1.4, hidden_layer_sizes=(120,90,40,20), random_state=1, max_iter=1800)\n",
            "\n",
            "print(np.mean(cross_val_score(regr, train_X, train_y, cv=3, n_jobs=12)))\n",
            "\n",
            "bestSize = 0\n",
            "bestAvg = 0\n",
            "\n",
            "#for i in range(1, 50):\n",
            "#  score = cross_val_score(MLPRegressor(activation='relu', alpha=1e-05, hidden_layer_sizes=(i,), random_state=1, max_iter=300), train_X, train_y, cv=5, n_jobs=4)\n",
            "#  if(np.mean(score) > bestAvg):\n",
            "#    bestAvg = np.mean(score)\n",
            "#    bestSize = i\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 34,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "MLPRegressor(activation='logistic', alpha=1.4, hidden_layer_sizes=(40, 40, 20),\n",
                     "             max_iter=1800, random_state=1)"
                  ]
               },
               "execution_count": 34,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "regr.fit(train_X, train_y)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 36,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "10061 10061\n",
                  "             Id   Predicted\n",
                  "0      22267382  158.040352\n",
                  "1      27551517   85.749535\n",
                  "2      27016367  704.952011\n",
                  "3      20917330  118.332212\n",
                  "4      21531318   74.980994\n",
                  "...         ...         ...\n",
                  "10056  10232344  173.181256\n",
                  "10057  16667077   78.762012\n",
                  "10058  13219269   68.163838\n",
                  "10059  12848383  401.725359\n",
                  "10060  17281121   95.820462\n",
                  "\n",
                  "[10061 rows x 2 columns]\n"
               ]
            }
         ],
         "source": [
            "test_df = pd.read_csv('test.csv')\n",
            "test_ids, test_X = [], []\n",
            "for (idx, row) in test_df.iterrows():\n",
            "  feature = create_feature(row, test_df)\n",
            "  test_ids.append(row.id)\n",
            "  test_X.append(feature)\n",
            "test_y = regr.predict(test_X)\n",
            "print(len(test_ids), len(test_X))\n",
            "\n",
            "\n",
            "output_df = pd.DataFrame()\n",
            "output_df['Id'] = test_ids\n",
            "output_df['Predicted'] = test_y\n",
            "output_df.to_csv('mlpregressor_prediction.csv', index=False)\n",
            "print(output_df)\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 57,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "0.5015955766426005\n"
               ]
            }
         ],
         "source": [
            "\n",
            "scaler_X = StandardScaler().fit(train_X)\n",
            "scaler_X.transform(train_X)\n",
            "\n",
            "# With standardized X\n",
            "# Setting up the neural net classifier\n",
            "regr = MLPRegressor(activation='logistic', alpha=1.4, hidden_layer_sizes=(120,100,30), random_state=1, max_iter=1800)\n",
            "\n",
            "print(np.mean(cross_val_score(regr, train_X, train_y, cv=5, n_jobs=12)))\n",
            "\n",
            "bestSize = 0\n",
            "bestAvg = 0\n",
            "\n",
            "#for i in range(1, 50):\n",
            "#  score = cross_val_score(MLPRegressor(activation='relu', alpha=1e-05, hidden_layer_sizes=(i,), random_state=1, max_iter=300), train_X, train_y, cv=5, n_jobs=4)\n",
            "#  if(np.mean(score) > bestAvg):\n",
            "#    bestAvg = np.mean(score)\n",
            "#    bestSize = i\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "0.5515102997703232\n"
               ]
            }
         ],
         "source": [
            "\n",
            "\n",
            "# This cell is used for cross validation of the model\n",
            "scaler_X = StandardScaler().fit(train_X)\n",
            "scaler_X.transform(train_X)\n",
            "\n",
            "# With standardized X\n",
            "# Setting up the neural net classifier\n",
            "regr = MLPRegressor(activation='logistic', alpha=1.4, hidden_layer_sizes=(180,120,30), random_state=1, max_iter=1800)\n",
            "\n",
            "print(np.mean(cross_val_score(regr, train_X, train_y, cv=5, n_jobs=12)))\n",
            "\n",
            "bestSize = 0\n",
            "bestAvg = 0\n",
            "\n",
            "#for i in range(1, 50):\n",
            "#  score = cross_val_score(MLPRegressor(activation='relu', alpha=1e-05, hidden_layer_sizes=(i,), random_state=1, max_iter=300), train_X, train_y, cv=5, n_jobs=4)\n",
            "#  if(np.mean(score) > bestAvg):\n",
            "#    bestAvg = np.mean(score)\n",
            "#    bestSize = i\n"
         ]
      }
   ],
   "metadata": {
      "interpreter": {
         "hash": "b8c7240f1d94b58b7ad391c7d1cda0d4e35465f6e237b27e5c4b4739ef98f4ab"
      },
      "kernelspec": {
         "display_name": "Python 3.9.7 ('base')",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.7"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
